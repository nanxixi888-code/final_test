# part2_trajectory_prediction.py

import numpy as np
from pathlib import Path
from typing import List
from rollout_loader import load_rollouts, Rollout 

# --- 我们现在 *重新* 导入 DatasetSplitter ---
from common_utilities import DatasetSplitter, FeatureNormalizer, ModelEvaluator, ModelPersistence
from machine_learning_utilities import DatasetPreparator, MLPRegressorTrainer, RandomForestRegressorTrainer
from visualization_utilities import TrajectoryPredictionVisualizer

# --- 新增：我们需要 sklearn 来正确地划分 Rollout 列表 ---
from sklearn.model_selection import train_test_split


class TrajectoryPredictionPipeline:
    
    def __init__(self):
        
        # --- 修改：删除了 self.input_features 和 self.output_targets ---
        # 它们现在被正确地隔离在 self.splits 字典中
        
        self.rollouts = { # 存储 Rollout 列表
            'train': [],
            'validation': [],
            'test': []
        }
        self.splits = {} # 存储 X_train, Y_train, X_val, Y_val ...
        self.mlp_normalized_data = {}
        self.mlp_scalers = {}
        self.mlp_model = None
        self.mlp_evaluation_results = {}
        self.rf_model = None
        self.rf_evaluation_results = {}
        
    # --- 修改 1: 替换 'load_and_prepare_dataset' ---
    # 新函数：加载所有 rollouts, 然后 *按文件* 拆分
    def load_and_split_rollouts(self, num_files=200, train_size=0.7, val_size=0.15, test_size=0.15):
        
        print(f"\nLoading data from rollouts...")
        
        SCRIPT_DIR = Path(__file__).resolve().parent
        
        # 自动加载 0 到 (num_files-1) 的所有 .pkl
        rollout_indices = list(range(num_files))
        
        try:
            loaded_rollouts = load_rollouts(indices=rollout_indices, directory=SCRIPT_DIR)
        except FileNotFoundError:
            print(f"Error: 找不到数据文件 (例如: data_0.pkl)。")
            print(f"请先(重新)运行我们修改后的 data_generator.py 来生成 {num_files} 个文件。")
            self.splits = None # 设置失败标志
            return self
        except ValueError as e:
            print(f"Error: 加载数据失败。请确保你的 .pkl 文件是最新的。")
            print(f"详细错误: {e}")
            self.splits = None # 设置失败标志
            return self

        if not loaded_rollouts:
             print("Error: 未加载到任何 rollout 文件。")
             self.splits = None
             return self
             
        print(f"Successfully loaded {len(loaded_rollouts)} rollout files.")

        # --- 这是正确的划分逻辑 ---
        print("Splitting *rollout list* into training, validation, and test sets...")

        # 确保比例总和为 1
        if train_size + val_size + test_size != 1.0:
            print(f"Error: 划分比例总和 ({train_size + val_size + test_size}) 不等于 1.0。")
            self.splits = None
            return self

        # 第一次拆分: 训练集 vs (验证集 + 测试集)
        train_rollouts, temp_rollouts = train_test_split(
            loaded_rollouts, 
            test_size=(val_size + test_size), 
            random_state=42
        )
        
        # 计算第二次拆分的比例 (例如 0.15 / (0.15 + 0.15) = 0.5)
        test_ratio_in_temp = test_size / (val_size + test_size)
        
        # 第二次拆分: 验证集 vs 测试集
        val_rollouts, test_rollouts = train_test_split(
            temp_rollouts,
            test_size=test_ratio_in_temp,
            random_state=42
        )
        
        self.rollouts['train'] = train_rollouts
        self.rollouts['validation'] = val_rollouts
        self.rollouts['test'] = test_rollouts
        
        print(f"Training rollouts: {len(self.rollouts['train'])}")
        print(f"Validation rollouts: {len(self.rollouts['validation'])}")
        print(f"Test rollouts: {len(self.rollouts['test'])}")
        
        return self

    # --- 修改 2: 替换 'split_dataset_into_train_validation_test' ---
    # 新函数：*分别* 为 train, val, test 准备 X 和 Y 数据
    def prepare_datasets_from_splits(self):
        print("\nPreparing datasets by extracting features/targets from rollout splits...")
        
        try:
            # 分别为训练集准备 X, Y
            input_train, output_train = DatasetPreparator.extract_trajectory_prediction_features_and_targets(
                self.rollouts['train']
            )
            # 分别为验证集准备 X, Y
            input_val, output_val = DatasetPreparator.extract_trajectory_prediction_features_and_targets(
                self.rollouts['validation']
            )
            # 分别为测试集准备 X, Y
            input_test, output_test = DatasetPreparator.extract_trajectory_prediction_features_and_targets(
                self.rollouts['test']
            )
        except Exception as e:
            print("错误：在 DatasetPreparator 中提取 X 和 Y 时发生错误。")
            print(f"详细信息: {e}")
            print("请检查 'machine_learning_utilities.py' 中的 'extract_trajectory_prediction_features_and_targets' 函数。")
            print("确保它能正确处理 rollouts 列表, 并且知道如何找到 'final target position'。")
            self.splits = None
            return self

        self.splits = {
            'input_train': input_train,
            'input_validation': input_val,
            'input_test': input_test,
            'output_train': output_train,
            'output_validation': output_val,
            'output_test': output_test
        }
        
        print(f"Training set size (X, Y): {input_train.shape}, {output_train.shape}")
        print(f"Validation set size (X, Y): {input_val.shape}, {output_val.shape}")
        print(f"Test set size (X, Y): {input_test.shape}, {output_test.shape}")
        
        return self

    # ---
    # --- 从这里开始，所有后续函数 (prepare_mlp_normalized_data, train_mlp_model, ...)
    # --- 都不需要任何修改，因为它们已经正确地从 'self.splits' 字典中读取数据了。
    # ---
    
    def prepare_mlp_normalized_data(self):
        
        print("\n" + "=" * 60)
        print("Preparing Data for MLP Regressor")
        print("=" * 60)
        
        print("\nNormalizing input features for MLP...")
        # 归一化器 (Scaler) *只* 在训练集上 'fit'，这是正确的
        (input_train_norm, input_val_norm, input_test_norm, 
         feature_scaler) = FeatureNormalizer.normalize_with_standard_scaler(
            self.splits['input_train'],
            self.splits['input_validation'],
            self.splits['input_test']
        )
        
        print("Normalizing output targets for MLP...")
        (output_train_norm, output_val_norm, output_test_norm, 
         target_scaler) = FeatureNormalizer.normalize_with_standard_scaler(
            self.splits['output_train'],
            self.splits['output_validation'],
            self.splits['output_test']
        )
        
        self.mlp_normalized_data = {
            'input_train': input_train_norm,
            'input_validation': input_val_norm,
            'input_test': input_test_norm,
            'output_train': output_train_norm,
            'output_validation': output_val_norm,
            'output_test': output_test_norm
        }
        
        self.mlp_scalers = {
            'feature_scaler': feature_scaler,
            'target_scaler': target_scaler
        }
        
        return self
    
    def train_mlp_model(self):
        print("\nTraining MLP Regressor with hyperparameter tuning...")
        print("This may take several minutes...")
        self.mlp_model = MLPRegressorTrainer.train_with_grid_search_cross_validation(
            self.mlp_normalized_data['input_train'],
            self.mlp_normalized_data['output_train']
        )
        return self
    
    def evaluate_mlp_model(self):
        print("\nEvaluating MLP model on different datasets...")
        predicted_train, actual_train, metrics_train = ModelEvaluator.evaluate_model_with_metrics(
            self.mlp_model, self.mlp_normalized_data['input_train'], self.mlp_normalized_data['output_train'],
            "MLP Training", self.mlp_scalers['target_scaler'], is_normalized=True
        )
        predicted_validation, actual_validation, metrics_validation = ModelEvaluator.evaluate_model_with_metrics(
            self.mlp_model, self.mlp_normalized_data['input_validation'], self.mlp_normalized_data['output_validation'],
            "MLP Validation", self.mlp_scalers['target_scaler'], is_normalized=True
        )
        predicted_test, actual_test, metrics_test = ModelEvaluator.evaluate_model_with_metrics(
            self.mlp_model, self.mlp_normalized_data['input_test'], self.mlp_normalized_data['output_test'],
            "MLP Test", self.mlp_scalers['target_scaler'], is_normalized=True
        )
        self.mlp_evaluation_results = {
            'train': {'predicted': predicted_train, 'actual': actual_train, 'metrics': metrics_train},
            'validation': {'predicted': predicted_validation, 'actual': actual_validation, 'metrics': metrics_validation},
            'test': {'predicted': predicted_test, 'actual': actual_test, 'metrics': metrics_test}
        }
        return self
    
    def train_random_forest_model(self):
        print("\n" + "=" * 60)
        print("Training Random Forest Regressor")
        print("=" * 60)
        print("\nTraining Random Forest Regressor with hyperparameter tuning...")
        print("This may take several minutes...")
        # 随机森林不需要归一化的数据，所以我们使用 'self.splits'
        self.rf_model = RandomForestRegressorTrainer.train_with_grid_search_cross_validation(
            self.splits['input_train'],
            self.splits['output_train']
        )
        return self
    
    def evaluate_random_forest_model(self):
        print("\nEvaluating Random Forest model on different datasets...")
        predicted_train, actual_train, metrics_train = ModelEvaluator.evaluate_model_with_metrics(
            self.rf_model, self.splits['input_train'], self.splits['output_train'],
            "Random Forest Training", is_normalized=False
        )
        predicted_validation, actual_validation, metrics_validation = ModelEvaluator.evaluate_model_with_metrics(
            self.rf_model, self.splits['input_validation'], self.splits['output_validation'],
            "Random Forest Validation", is_normalized=False
        )
        predicted_test, actual_test, metrics_test = ModelEvaluator.evaluate_model_with_metrics(
            self.rf_model, self.splits['input_test'], self.splits['output_test'],
            "Random Forest Test", is_normalized=False
        )
        self.rf_evaluation_results = {
            'train': {'predicted': predicted_train, 'actual': actual_train, 'metrics': metrics_train},
            'validation': {'predicted': predicted_validation, 'actual': actual_validation, 'metrics': metrics_validation},
            'test': {'predicted': predicted_test, 'actual': actual_test, 'metrics': metrics_test}
        }
        return self
    
    def print_model_comparison_summary(self):
        # (无变化)
        metrics_list = [
            [
                self.mlp_evaluation_results['train']['metrics'],
                self.mlp_evaluation_results['validation']['metrics'],
                self.mlp_evaluation_results['test']['metrics']
            ],
            [
                self.rf_evaluation_results['train']['metrics'],
                self.rf_evaluation_results['validation']['metrics'],
                self.rf_evaluation_results['test']['metrics']
            ]
        ]
        model_names = ['MLP Regressor', 'Random Forest Regressor']
        dataset_names = ['Training', 'Validation', 'Test']
        ModelEvaluator.print_metrics_comparison_table(metrics_list, model_names, dataset_names)
        return self
    
    def generate_comparison_visualizations(self, number_of_joints=7):
        # (无变化)
        print("\nGenerating comparison visualizations...")
        TrajectoryPredictionVisualizer.plot_comparison_for_all_joints(
            self.mlp_evaluation_results['test']['predicted'],
            self.rf_evaluation_results['test']['predicted'],
            self.mlp_evaluation_results['test']['actual'],
            number_of_joints,
            save_prefix='part2_trajectory_prediction'
        )
        return self
    
    def save_trained_models(self, mlp_filename='part2_trajectory_prediction_mlp_model.pkl',
                           rf_filename='part2_trajectory_prediction_rf_model.pkl'):
        # (无变化)
        ModelPersistence.save_model_with_scalers(
            self.mlp_model,
            self.mlp_scalers['feature_scaler'],
            self.mlp_scalers['target_scaler'],
            mlp_filename
        )
        ModelPersistence.save_model_only(
            self.rf_model,
            rf_filename
        )
        return self


def main():
    
    print("Part 2: Trajectory Prediction using MLP and Random Forest")
    print("=" * 60)
    
    # --- 修改 3: 更新 main 函数中的调用流程 ---
    trajectory_prediction_pipeline = TrajectoryPredictionPipeline()
    
    # 步骤 1: 加载并按文件(Rollout)拆分
    # 假设我们生成了 200 个文件，按 70% 训练 (140), 15% 验证 (30), 15% 测试 (30) 拆分
    trajectory_prediction_pipeline.load_and_split_rollouts(
        num_files=200, 
        train_size=0.7, 
        val_size=0.15, 
        test_size=0.15
    )
    
    if trajectory_prediction_pipeline.splits is None:
        print("\nStopping pipeline due to data loading/preparation error.")
        return

    # 步骤 2: 分别从拆分好的 Rollouts 中提取 X 和 Y
    trajectory_prediction_pipeline.prepare_datasets_from_splits()

    if trajectory_prediction_pipeline.splits is None:
        print("\nStopping pipeline due to dataset preparation error.")
        return
        
    # --- 步骤 3: 删除 'split_dataset_into_train_validation_test' 的调用 ---
    # (这一步现在由 步骤 1 和 2 完成了)
    
    # --- 后续所有步骤都与你组员的原始代码完全相同 ---
    
    trajectory_prediction_pipeline.prepare_mlp_normalized_data()
    
    trajectory_prediction_pipeline.train_mlp_model()
    
    trajectory_prediction_pipeline.evaluate_mlp_model()
    
    trajectory_prediction_pipeline.train_random_forest_model()
    
    trajectory_prediction_pipeline.evaluate_random_forest_model()
    
    trajectory_prediction_pipeline.print_model_comparison_summary()
    
    trajectory_prediction_pipeline.generate_comparison_visualizations(number_of_joints=7)
    
    trajectory_prediction_pipeline.save_trained_models(
        mlp_filename='part2_trajectory_prediction_mlp_model.pkl',
        rf_filename='part2_trajectory_prediction_rf_model.pkl'
    )
    
    print("\n" + "=" * 60)
    print("Part 2 completed successfully!")
    print("=" * 60)


if __name__ == '__main__':
    main()
